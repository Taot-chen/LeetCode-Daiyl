## 使用2GB内存在 20亿个整数中找出出现次数最多的整数

In One Word:

* 数据分割（哈希分流）--> 词频统计（单文件处理）--> 结果合并（全局比较）


----------


### 1 具体说明

首先，32bit 的 int 数可以表示的数据最大值大于 20亿。

考虑两个极端情况：

* 20 亿个数是同一个数，此时需要一个 int32 来表示目标数字，还需要一个 uint32 来表示计数，此时需要 8Byte
* 当 20 亿个数各不相同的时候，此时需要考虑，每个数字的统计，需要 8Byte 内存，那么 20亿个数，需要至少 `2 * 10^9 * 8 Byte --> 16GB` 内存，因此 2GB 内存不够用，考虑对数据拆分处理


#### 1.1 数据分割（哈希分流）

利用哈希函数的均匀性，将 20 亿个整数分散到多个小文件中，确保相同数字一定被分配到同一文件，且每个文件的数据量在内存可处理范围内。

选择哈希函数的原因是哈希函数的均匀性，并且对于相同的数字哈希后的值相同，方便将相同数字分配到同一文件。

* 选择哈希函数：`hash(num) = num % N`，其中 N 为小文件数量

* 遍历大文件，将每个数写入对应的子文件

* 文件数量确定：
    * 每个整数占 4B（32 位），频次统计占 4B，每条记录共 8B
    * 考虑尽量用尽 int32 的数据范围，最大可表示 2^31-1，略大于 20亿，2GB 内存共 2^31B
    * 2GB 内存可处理 `2^31/8 = 2^28` 个数字
    * 最少文件数：20亿/2^28
    考虑到哈希函数的均匀性，这里取 16

```python
# Step 1: 哈希分流
files = [open(f"file_{i}.txt", "w") for i in range(n_files)]
for num in big_file:
    file_id = hash(num) % n_files  # 哈希取模
    files[file_id].write(num + "\n")
```

#### 1.2 词频统计

依次处理每个小文件，在内存中用哈希表统计该文件内所有数字的出现频次。

读取一个子文件，使用哈希表（如 Python 字典）记录每个数的出现次数。找出该文件中出现次数最多的数及其频次，保存结果。

分成 16 个文件的时候，每个小文件的数据量 ≤ 1.25 亿条记录，哈希表内存占用 ≤ 1.25亿 × 8B = 1GB，远低于 2GB 限制

```python
# Step 2: 单文件统计
local_maxes = []
for i in range(n_files):
    freq_map = {}
    for num in read_file(f"file_{i}.txt"):
        freq_map[num] = freq_map.get(num, 0) + 1
    max_num = max(freq_map, key=freq_map.get)
    local_maxes.append((max_num, freq_map[max_num]))
```



#### 1.3 结果合并（全局比较）

从所有子文件的局部最大值中选出全局最大值。收集 16 个子文件中的最大值（每个文件输出一个候选数及频次），比较这 16 个候选数的频次，选出最高者即为全局出现次数最多的数。

```python
# Step 3: 全局比较
global_max = max(local_maxes, key=lambda x: x[1])
return global_max[0]  # 返回出现次数最多的数
```


### 2 优化机会

* 哈希分流：
    * 保均匀性（如取模运算的底数选质数），避免数据倾斜导致单个文件过大
    * 若某个子文件过大，可二次哈希分割（拆成更小的子文件）

* 内存不足时：若某文件的不同数值超过内存上限（如 2.68 亿），可临时写入磁盘或二次分流

需两轮遍历（分流 + 统计），效率为 O(n)



### 3 实现

python 实现：

```python
import os
import shutil
from collections import defaultdict
from typing import List, Tuple
import time

def hash_func(num: int, n_files: int) -> int:
    return abs(hash(str(num))) % n_files    # 使用字符串哈希保证均匀性

def split_large_file(input_path: str, n_files: int, temp_dir: str) -> None:
    output_files = [open(os.path.join(temp_dir, f"part_{i}.txt"), 'w') for i in range(n_files)]

    # 逐行读取大文件并分流
    with open(input_path, 'r') as f:
        for line in f:
            num = int(line.strip())
            file_id = hash_func(num, n_files)
            output_files[file_id].write(f"{num}\n")

    for f in output_files:
        f.close()

def process_chunk(file_path: str) -> Tuple[int, int]:
    freq_map = defaultdict(int)
    max_num = None
    max_count = 0

    with open(file_path, 'r') as f:
        for line in f:
            num = int(line.strip())
            freq_map[num] += 1
            if freq_map[num] > max_count:
                max_num = num
                max_count = freq_map[num]
    return (max_num, max_count) if max_num is not None else (0, 0)


def find_global_max(temp_dir: str, n_files: int) -> Tuple[int, int]:
    global_max_num = 0
    global_max_count = 0
    for i in range(n_files):
        file_path = os.path.join(temp_dir, f"part_{i}.txt")
        if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:
            continue  # 跳过空文件
        num, count = process_chunk(file_path)
        if count > global_max_count:
            global_max_num = num
            global_max_count = count
    return global_max_num, global_max_count

def find_most_frequent(input_path: str, n_files: int = 256) -> Tuple[int, int]:
    temp_dir = "tmp_chunks"
    os.makedirs(temp_dir, exist_ok=True)

    try:
        # 哈希分流
        split_large_file(input_path, n_files, temp_dir)

        # 处理每个分块并找出全局最大值
        result = find_global_max(temp_dir, n_files)
        return result
    finally:
        # 清理临时文件
        shutil.rmtree(temp_dir)

# 测试用例
def generate_test_data(output_path: str, n: int = 1000000) -> None:
    import random
    with open(output_path, 'w') as f:
        # 生成10个高频数字（占总数据10%）
        for _ in range(n // 10):
            f.write(f"{random.randint(1, 10)}\n")
        # 生成随机数字
        for _ in range(n - n // 10):
            f.write(f"{random.randint(1, n)}\n")

if __name__ == "__main__":
    start = time.time()
    generate_test_data("test_data.txt", n=1000000)
    # 执行查找
    number, count = find_most_frequent("test_data.txt")
    print(f"出现次数最多的整数是: {number}, 出现次数: {count}")
    print(f"耗时：{time.time() - start}s")
```



c++ 实现：

```cpp
#include <iostream>
#include <fstream>
#include <unordered_map>
#include <vector>
#include <filesystem>
#include <cstdlib>
#include <cmath>
#include <climits>
#include <chrono>

namespace fs = std::filesystem;

size_t hash_func(int num, size_t n_files) {
    return static_cast<size_t>(std::hash<int>{}(num)) % n_files;
}

void split_large_file(const std::string& input_path, size_t n_files, const std::string& temp_dir) {
    std::vector<std::ofstream> out_files;
    for (size_t i = 0; i < n_files; i++) {
        out_files.emplace_back(temp_dir + "/chunk_" + std::to_string(i) + ".bin", std::ios::binary);
    }

    std::ifstream in_file(input_path, std::ios::binary);
    if (!in_file) throw std::runtime_error("无法打开输入文件");

    int num;
    while (in_file.read(reinterpret_cast<char*>(&num), sizeof(int))) {
        size_t file_id = hash_func(num, n_files);
        out_files[file_id].write(reinterpret_cast<const char*>(&num), sizeof(int));
    }

    for(auto& f: out_files) f.close();
}

std::pair<int, size_t> process_chunk(const std::string& file_path) {
    std::ifstream in_file(file_path, std::ios::binary | std::ios::ate);
    if (!in_file || in_file.tellg() == 0) return {0, 0};
    in_file.close();

    in_file.open(file_path, std::ios::binary);
    std::unordered_map<int, size_t> freq_map;
    int num;
    int max_num = 0;
    size_t max_count = 0;

    while (in_file.read(reinterpret_cast<char*>(&num), sizeof(int))) {
        size_t count = ++freq_map[num];
        if (count > max_count) {
            max_count = count;
            max_num = num;
        }
    }
    return {max_num, max_count};
}

std::pair<int, size_t> find_most_frequent(const std::string& input_path, size_t n_files = 256) {
    std::string temp_dir = "tmp_chunks";
    if (!fs::exists(temp_dir)) fs::create_directory(temp_dir);

    try {
        split_large_file(input_path, n_files, temp_dir);
        int global_max_num = 0;
        size_t global_max_count = 0;

        for (size_t i = 0; i < n_files; i++) {
            std::string chunk_path = temp_dir + "/chunk_" + std::to_string(i) + ".bin";
            auto [num, count] = process_chunk(chunk_path);

            if (count > global_max_count) {
                global_max_count = count;
                global_max_num = num;
            }
        }

        return {global_max_num, global_max_count};
    } catch (...) {
        fs::remove_all(temp_dir);
        throw;
    }
    fs::remove_all(temp_dir);
    return {0, 0};
}


// 测试用例
void generate_test_data(const std::string& output_path, size_t n = 1000000) {
    std::ofstream out_file(output_path, std::ios::binary);
    std::srand(42);
    for (size_t i = 0; i < n; ++i) {
        int num = (i < n / 10) ? std::rand() % 10 : std::rand() % n;
        out_file.write(reinterpret_cast<const char*>(&num), sizeof(int));
    }
}

int main() {
    auto start = std::chrono::high_resolution_clock::now();
    // 生成测试数据
    generate_test_data("test_data.bin", 100000000);
    
    // 执行查找
    auto [num, count] = find_most_frequent("test_data.bin", 16);
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
    std::cout << "出现次数最多的整数: " << num << ", 出现次数： " << count << std::endl;
    std::cout << "耗时：" << duration.count() / 1000000.0 << "s" << std::endl;;
    return 0;
}
```



100000000 个数字结果：

```bash
# python
出现次数最多的整数是: 4, 出现次数: 1003185
耗时：243.7758595943451s

# c++ O0 优化
出现次数最多的整数: 5, 出现次数： 1001878
耗时：59.9544s

# c++ O3 优化
出现次数最多的整数: 5, 出现次数： 1001878
耗时：33.1885s
```